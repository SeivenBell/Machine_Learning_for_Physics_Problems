## PHYS449 Machine Learning in Physics
Name: Severyn Balaniuk
Student ID: 21050967

# Assignment 3: Variational Autoencoder for Even MNIST
Description of Task and Solution:

- Task Overview:
This assignment's primary objective was to develop a Variational Autoencoder (VAE) capable of generating images of 
even digits. The challenge involved working with a subset of the MNIST dataset, consisting exclusively of even-numbered
 digits (0, 2, 4, 6, 8) that had been resized to 14x14 pixels. The task required the implementation of a VAE with 
 convolutional layers in its encoder, aiming to learn a latent representation of the even digits and generate new digit 
 images from this learned space.

# Solution Strategy
Data Preparation:

The MNIST dataset, containing images of handwritten digits, was filtered to include only even numbers.
Each image was downsized to 14x14 pixels to reduce computational complexity while retaining sufficient detail.
Images were normalized to have pixel values between 0 and 1, making them suitable for processing by the VAE.

Model Architecture:

The VAE was designed with convolutional layers in the encoder part to effectively capture the spatial hierarchies in
images.

The decoder part of the VAE was tasked with reconstructing the digits from the latent space representations.
The model architecture was tuned to balance the reconstruction quality and the regularity of the latent space.
Training Process:

The VAE was trained using a loss function combining reconstruction loss (how well the output matches the input) and 
the KL divergence (a measure of how much the learned latent distribution deviates from a standard normal distribution).
Hyperparameters such as learning rate, batch size, and epochs were finely tuned for optimal training performance.

Output Generation:

After training, the model was used to generate new digit images by sampling from the latent space and decoding these 
samples back to image space.
These generated images provided a qualitative assessment of the model's performance.

- How AI Helped:

ChatGPT assisted in designing the architecture of the VAE, providing explanations on how functions
 and convolutional layers work and can be tuned. It also helped me understand different VAE implementations I found on GitHub, 
 clarifying their context and application. Most importantly, AI was instrumental in handling debugging, offering insights into 
 the errors I encountered. This guidance was crucial in reducing unnecessary stress and frustration during the development, 
 training, and evaluation stages of the model.

## How to Run the Code

Set-Up: Ensure all required libraries are installed. 
And make sure you are in Assignment_3 folder to run the command 

Running the Script: Use the following command to run main.py:
sh

# python main.py -o result_dir -n 100 -v


-o result_dir: Directory to save output PDF files (loss plot and digit samples).
-n 100: Number of digit samples to generate.
-v: is a verbose mode, i will give additional information about the training.

Help Command: For additional information, use:
sh

# python main.py --help

-h, --help
Functionality: When you run python main.py --help, the script prints out a help message. This message is automatically generated by argparse based on the arguments you've defined in your script.
Output: The help message typically includes:
A brief usage instruction (usage: main.py [-h] -o OUTPUT_DIR -n NUM_SAMPLES [-v]), which shows the expected syntax for running the script, including all possible flags and options.
A description of the script's purpose (e.g., "Train a VAE on Even MNIST Numbers"), if you've provided one in the description argument of ArgumentParser.
A list of options, each of which represents a command-line argument that the script accepts. For each option, a description is shown, which is the text you've provided in the help argument of add_argument.
Options in Your Script
-o OUTPUT_DIR, --output_dir OUTPUT_DIR:

Description: "Directory to save output files"
Functionality: This argument is required (required=True) and specifies the directory where the output files (like generated digit samples and loss plots) will be saved.
-n NUM_SAMPLES, --num_samples NUM_SAMPLES:

Description: "Number of digit samples to generate"
Functionality: This argument is also required and determines the number of digit images the script will generate after training the VAE.
-v, --verbose:

Description: "Enable verbose output"
Functionality: This is an optional flag. If included, it enables verbose mode, making the script print additional details about its progress during execution, particularly during training.


Input Data Format:

The program expects a CSV file containing the even MNIST dataset.
The dataset should consist of 14x14 pixel images of even digits, flattened into rows.

Output:
Generated digit samples saved as PDF files (1-100) in the specified output directory.
A PDF file (loss.pdf) showing the training and test loss over epochs.

# Dependencies

Python 3.8 or newer
PyTorch 1.8.0 or newer
Matplotlib 3.3.2 or newer
NumPy 1.19.2 or newer
Pandas 1.1.3 or newer

*full requirements below if needed or in separate file*

Usage Documentation for the Configuration File
The provided configuration file (config.json) is a key component of setting up and running the Variational Autoencoder (VAE) for generating even digits from the MNIST dataset. This file allows you to easily adjust various parameters of the VAE without modifying the core script. Below is a breakdown of each parameter in the configuration file and its role in the model's setup and training process.

Configuration File Structure
The params.json file is structured in JSON format as follows:

json

{
    "data_path": "data/even_mnist.csv",
    "epochs": 10,
    "lr": 0.001,
    "latent_dim": 50,
    "batch_size": 64,
    "test_size": 0.2
}

# Parameters Explained:

data_path: (str) Specifies the path to the dataset. In this case, "data/even_mnist.csv" is the file containing the even MNIST digits. The path should be relative to the main script or an absolute path can be provided.

epochs: (int) Determines the number of training cycles the model will go through. An epoch is one complete pass through the entire dataset. Here, it is set to 10, meaning the entire dataset will be passed through the VAE ten times during training.

lr (Learning Rate): (float) This is a crucial hyperparameter in training neural networks. It defines the step size during the optimization process. A value of 0.001 is often a good starting point for many models.

latent_dim (Latent Dimension): Refers to the size of the latent space in the VAE. This number (50 in the given configuration) represents the dimensionality of the encoded representations of the input data.

batch_size: (integer) This parameter controls the number of data samples that the model will process before updating the model parameters. A batch size of 64 means that 64 data samples will be used to compute the loss and update the model during each iteration of an epoch.

test_size: (float) Indicates the proportion of the dataset to be used as test data. A value of 0.2 means that 20% of the dataset is reserved for testing the modelâ€™s performance, while the remaining 80% is used for training.

# Requirements:

absl-py==1.4.0
aiohttp==3.8.4
aiosignal==1.3.1
alembic==1.12.1
asttokens==2.2.1
async-timeout==4.0.2
attrs==23.1.0
backcall==0.2.0
beautifulsoup4==4.12.2
blinker==1.7.0
blis==0.7.9
Bottleneck==1.3.7
cachetools==5.3.1
catalogue==2.0.8
certifi==2022.12.7
charset-normalizer==3.1.0
click==8.1.4
cloudpickle==2.2.1
colorama==0.4.6
comm==0.1.3
confection==0.1.0
contourpy==1.0.7
cycler==0.11.0
cymem==2.0.7
databricks-cli==0.18.0
debugpy==1.6.7
decorator==5.1.1
dill==0.3.7
distlib==0.3.6
docker==6.1.3
entrypoints==0.4
executing==1.2.0
fastai==2.7.13
fastcore==1.5.29
fastdownload==0.0.7
fastprogress==1.0.3
filelock==3.10.0
Flask==3.0.0
flatbuffers==23.3.3
fonttools==4.39.2
frozenlist==1.3.3
gitdb==4.0.11
GitPython==3.1.40
google-auth==2.22.0
google-auth-oauthlib==1.0.0
greenlet==3.0.1
grpcio==1.57.0
h5py==3.8.0
icecream==2.1.3
idna==3.4
importlib-metadata==6.8.0
ipykernel==6.23.1
ipython==8.14.0
itsdangerous==2.1.2
jedi==0.18.2
Jinja2==3.1.2
joblib==1.2.0
jupyter_client==8.2.0
jupyter_core==5.3.0
kiwisolver==1.4.4
langcodes==3.3.0
libclang==16.0.0
Mako==1.3.0
Markdown==3.4.4
MarkupSafe==2.1.2
matplotlib==3.7.1
matplotlib-inline==0.1.6
mlflow==2.8.1
mpmath==1.3.0
multidict==6.0.4
murmurhash==1.0.9
nest-asyncio==1.5.6
networkx==3.1
numexpr==2.8.7
numpy==1.24.2
nvidia-ml-py3==7.352.0
oauthlib==3.2.2
openai==0.27.7
opencv-python==4.7.0.72
packaging==23.0
pandas==2.0.3
parso==0.8.3
pathy==0.10.2
pbr==5.11.1
pickleshare==0.7.5
Pillow==9.4.0
platformdirs==3.1.1
ply==3.11
preshed==3.0.8
prompt-toolkit==3.0.38
protobuf==4.24.0
psutil==5.9.5
pure-eval==0.2.2
pyarrow==14.0.1
pyasn1==0.4.8
pyasn1-modules==0.3.0
pydantic==1.10.11
Pygments==2.15.1
PyJWT==2.8.0
pylatexenc==2.10
pyparsing==3.0.9
PyPDF2==3.0.1
python-dateutil==2.8.2
python-version==0.0.2
pytz==2023.3
pywin32==306
PyYAML==6.0
pyzmq==25.1.0
qiskit==0.44.2
qiskit-terra==0.25.2.1
querystring-parser==1.2.4
regex==2023.5.5
requests==2.28.2
requests-oauthlib==1.3.1
rsa==4.9
rustworkx==0.13.2
scikit-learn==1.3.2
scipy==1.10.1
seaborn==0.12.2
six==1.16.0
smart-open==6.3.0
smmap==5.0.1
soupsieve==2.5
spacy==3.6.0
spacy-legacy==3.0.12
spacy-loggers==1.0.4
SQLAlchemy==2.0.23
sqlparse==0.4.4
srsly==2.4.6
stack-data==0.6.2
stevedore==5.1.0
sympy==1.11.1
tabulate==0.9.0
tensorboard==2.14.0
tensorboard-data-server==0.7.1
tensorboard-plugin-wit==1.8.1
thinc==8.1.10
threadpoolctl==3.1.0
torch==2.0.0+cu117
torch-tb-profiler==0.4.1
torchaudio==2.0.1+cu117
torchinfo==1.8.0
torchsummary==1.5.1
torchvision==0.15.1
tornado==6.3.2
tqdm==4.65.0
traitlets==5.9.0
typer==0.9.0
typing_extensions==4.5.0
tzdata==2023.3
urllib3==1.26.15
virtualenv==20.21.0
virtualenvwrapper-win==1.2.7
waitress==2.1.2
wasabi==1.1.2
wcwidth==0.2.6
websocket-client==1.6.4
Werkzeug==3.0.1
wrapt==1.14.1
yarl==1.9.2
zipp==3.17.0
